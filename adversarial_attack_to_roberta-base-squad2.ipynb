{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9af6811",
   "metadata": {},
   "source": [
    "This notebook performs an evaluation of various adversarial attacks on a RoBERTa-based question-answering model fine-tuned on the SQuAD 2.0 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4722c",
   "metadata": {},
   "source": [
    "### Libraries loading\n",
    "\n",
    "Imports necessary libraries for natural language processing, machine learning, and data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9202f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05853332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy for grammatical error detection\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f7a86b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ferhatsarikaya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148ca9d",
   "metadata": {},
   "source": [
    "### Model and tokenizer loading\n",
    "\n",
    "It loads a pre-trained RoBERTa model and tokenizer fine-tuned on SQuAD 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5698d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepset/roberta-base-squad2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1c4e8",
   "metadata": {},
   "source": [
    "### Data loading function\n",
    "\n",
    "load_squad_data() reads JSON files containing SQuAD-format data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c557d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25dba97",
   "metadata": {},
   "source": [
    "### Answer prediction\n",
    "\n",
    "get_answer() uses the RoBERTa model to predict answers for given questions and contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76aa90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, context):\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b93fa",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "calculate_f1_score(): Calculates the F1 score between predicted and ground truth answers.<br />\n",
    "calculate_bleu_score(): Computes the BLEU score for predicted answers. <br />\n",
    "count_grammatical_errors(): Estimates grammatical errors in predicted answers using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a95d24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = word_tokenize(prediction.lower())\n",
    "    ground_truth_tokens = word_tokenize(ground_truth.lower())\n",
    "    \n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef76bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(prediction, ground_truth):\n",
    "    return sentence_bleu([word_tokenize(ground_truth.lower())], word_tokenize(prediction.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da27af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_grammatical_errors(text):\n",
    "    doc = nlp(text)\n",
    "    return len([token for token in doc if token.dep_ == 'ROOT']) - 1  # A rough estimate of grammatical errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b92e3",
   "metadata": {},
   "source": [
    "### Main evaluation function\n",
    "\n",
    "evaluate_attack() processes the dataset, generates predictions, and calculates various metrics for each question-answer pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6283eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack(data, attack_name):\n",
    "    results = []\n",
    "    for article in tqdm(data, desc=f\"Evaluating {attack_name}\"):\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                if qa['answers']:\n",
    "                    ground_truth = qa['answers'][0]['text']\n",
    "                    predicted_answer = get_answer(question, context)\n",
    "                    \n",
    "                    # Check if the ground truth is in the truncated context\n",
    "                    if ground_truth not in context[:512]:\n",
    "                        continue  # Skip this example as the answer is not in the truncated context\n",
    "                    \n",
    "                    exact_match = predicted_answer.lower() == ground_truth.lower()\n",
    "                    f1_score = calculate_f1_score(predicted_answer, ground_truth)\n",
    "                    bleu_score = calculate_bleu_score(predicted_answer, ground_truth)\n",
    "                    grammatical_errors = count_grammatical_errors(predicted_answer)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'attack': attack_name,\n",
    "                        'question': question,\n",
    "                        'context': context[:512],  # Truncate context for storage\n",
    "                        'ground_truth': ground_truth,\n",
    "                        'predicted_answer': predicted_answer,\n",
    "                        'exact_match': exact_match,\n",
    "                        'f1_score': f1_score,\n",
    "                        'bleu_score': bleu_score,\n",
    "                        'grammatical_errors': grammatical_errors\n",
    "                    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb96b7",
   "metadata": {},
   "source": [
    "### Attack evaluation loop\n",
    "\n",
    "The script iterates through different adversarial attack datasets (AddAny, AddSent, CEIA, DPAEG, TextFooler), evaluating the model's performance on each.<br /><br />\n",
    "Results for all attacks are collected in a list and then converted to a pandas DataFrame. After that, saves detailed results to a CSV file named \"roberta-base-squad2_adversarial_attack_results.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02a84275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating AddAny:   0%|                                | 0/442 [00:00<?, ?it/s]/Users/ferhatsarikaya/miniforge3/envs/new_tf_env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ferhatsarikaya/miniforge3/envs/new_tf_env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/ferhatsarikaya/miniforge3/envs/new_tf_env/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Evaluating AddAny: 100%|████████████████████| 442/442 [2:48:48<00:00, 22.92s/it]\n",
      "Evaluating AddSent: 100%|███████████████████| 442/442 [2:49:42<00:00, 23.04s/it]\n",
      "Evaluating CEIA: 100%|██████████████████████| 442/442 [2:46:13<00:00, 22.56s/it]\n",
      "Evaluating DPAEG: 100%|█████████████████████| 442/442 [2:47:51<00:00, 22.79s/it]\n",
      "Evaluating TextFooler: 100%|████████████████| 442/442 [2:51:29<00:00, 23.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# List of attack files\n",
    "attack_files = [\n",
    "    (\"SQuAD/squad-v2.0-addany.json\", \"AddAny\"),\n",
    "    (\"SQuAD/squad-v2.0-addsent.json\", \"AddSent\"),\n",
    "    (\"SQuAD/squad-v2.0-CEIA.json\", \"CEIA\"),\n",
    "    (\"SQuAD/squad-v2.0-dpaeg.json\", \"DPAEG\"),\n",
    "    (\"SQuAD/squad-v2.0-textfooler.json\", \"TextFooler\")\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for file_path, attack_name in attack_files:\n",
    "    data = load_squad_data(file_path)\n",
    "    results = evaluate_attack(data, attack_name)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"roberta-base-squad2_adversarial_attack_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed183af",
   "metadata": {},
   "source": [
    "### Summaries\n",
    "\n",
    "Calculates and prints summary statistics for each attack type. After that, save the summary statistics to another CSV file named \"roberta-base-squad2_adversarial_attack_summary.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2fbb8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Exact Match  F1 Score  BLEU Score  Avg Grammatical Errors  \\\n",
      "attack                                                                  \n",
      "AddAny         0.053941  0.849227    0.157224                0.005401   \n",
      "AddSent        0.053690  0.841340    0.154198                0.006270   \n",
      "CEIA           0.026497  0.862067    0.122997                0.002045   \n",
      "DPAEG          0.021695  0.738067    0.102114                0.007000   \n",
      "TextFooler     0.013394  0.492457    0.064835                0.011591   \n",
      "\n",
      "            Sample Size  \n",
      "attack                   \n",
      "AddAny            67945  \n",
      "AddSent           67945  \n",
      "CEIA              56724  \n",
      "DPAEG             56143  \n",
      "TextFooler        54351  \n"
     ]
    }
   ],
   "source": [
    "# Calculate and print summary statistics\n",
    "summary = df.groupby('attack').agg({\n",
    "    'exact_match': 'mean',\n",
    "    'f1_score': 'mean',\n",
    "    'bleu_score': 'mean',\n",
    "    'grammatical_errors': 'mean',\n",
    "    'attack': 'count'\n",
    "})\n",
    "summary.columns = ['Exact Match', 'F1 Score', 'BLEU Score', 'Avg Grammatical Errors', 'Sample Size']\n",
    "print(summary)\n",
    "\n",
    "# Save summary to CSV\n",
    "summary.to_csv(\"roberta-base-squad2_adversarial_attack_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e43868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (new_tf)",
   "language": "python",
   "name": "yeni_ortam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
